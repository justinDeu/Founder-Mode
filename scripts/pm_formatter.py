#!/usr/bin/env python3
"""
PM Integration Formatter for founder-mode

Converts feature suggestions from feature_identifier.py into founder-mode
compatible formats: prompt files, scope definitions, and CLI commands.
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Literal
import subprocess


# Template for generated prompt files
PROMPT_TEMPLATE = '''# Generated by direction-analyzer
# Suggestion: {suggestion_id}
# Type: {suggestion_type}
# Priority: {priority_score}

<objective>
{title}

{description}
</objective>

<context>
{context_with_evidence}

{file_references}
</context>

<requirements>
{requirements_from_action}
</requirements>

<output>
{output_files}
</output>

<verification>
{verification_steps}
</verification>

<completion_protocol>
When ALL tasks are complete and verified, output this EXACT line as your final output:

<verification>VERIFICATION_COMPLETE</verification>

If anything is incomplete or failing, output this EXACT line with your reason:

<verification>NEEDS_RETRY: [reason]</verification>
</completion_protocol>
'''


def format_as_prompt(suggestion: dict[str, Any]) -> str:
    """Generate a prompt file content from a suggestion."""
    suggestion_type = suggestion.get("type", "new")
    title = suggestion.get("title", "Untitled Task")
    description = suggestion.get("description", "")
    suggested_action = suggestion.get("suggested_action", "")
    evidence = suggestion.get("evidence", [])
    related_files = suggestion.get("related_files", [])
    priority = suggestion.get("priority_score", 0.0)
    suggestion_id = suggestion.get("id", "unknown")

    # Build context with evidence
    context_parts = []
    if evidence:
        context_parts.append("**Evidence:**")
        for ev in evidence[:5]:  # Limit to top 5
            context_parts.append(f"- {ev}")

    context_with_evidence = "\n".join(context_parts) if context_parts else "No specific evidence provided."

    # Build file references
    file_references = ""
    if related_files:
        file_refs = []
        for file_path in related_files[:10]:  # Limit to top 10 files
            file_refs.append(f"@{file_path}")
        file_references = "\n".join(file_refs)

    # Generate requirements from suggested action
    requirements_from_action = suggested_action or "Implement the described feature."

    # Infer output files based on type and related files
    output_files = infer_output_files(suggestion_type, related_files, suggested_action)

    # Generate verification steps
    verification_steps = generate_verification_steps(suggestion_type, related_files)

    return PROMPT_TEMPLATE.format(
        suggestion_id=suggestion_id,
        suggestion_type=suggestion_type,
        priority_score=priority,
        title=title,
        description=description,
        context_with_evidence=context_with_evidence,
        file_references=file_references,
        requirements_from_action=requirements_from_action,
        output_files=output_files,
        verification_steps=verification_steps
    )


def infer_output_files(
    suggestion_type: str,
    related_files: list[str],
    suggested_action: str
) -> str:
    """Infer likely output files based on suggestion type and context."""
    outputs = []

    if suggestion_type == "continuation":
        # Continue working on existing files
        if related_files:
            outputs.append("Modified files:")
            for f in related_files[:5]:
                outputs.append(f"  - {f}")
        outputs.append("\nNew tests for implemented functionality")

    elif suggestion_type == "debt":
        outputs.append("Refactored code with:")
        outputs.append("  - Improved structure")
        outputs.append("  - No functional changes")
        outputs.append("  - All tests passing")
        if related_files:
            outputs.append("\nModified files:")
            for f in related_files[:5]:
                outputs.append(f"  - {f}")

    elif suggestion_type == "unblock":
        outputs.append("Analysis document or decision record")
        outputs.append("Updated implementation plan")

    elif suggestion_type == "new":
        outputs.append("New implementation files")
        outputs.append("Corresponding test files")
        outputs.append("Updated documentation")

    else:
        outputs.append("Relevant implementation files")
        outputs.append("Test coverage")
        outputs.append("Documentation updates")

    return "\n".join(outputs) if outputs else "Implementation files and tests"


def generate_verification_steps(suggestion_type: str, related_files: list[str]) -> str:
    """Generate verification steps based on suggestion type."""
    steps = []

    if suggestion_type == "continuation":
        steps.append("1. All implemented features function correctly")
        steps.append("2. New tests cover the implementation")
        steps.append("3. No regressions in existing functionality")
        if related_files:
            steps.append("4. Related files are updated consistently")

    elif suggestion_type == "debt":
        steps.append("1. Code is cleaner and more maintainable")
        steps.append("2. All existing tests still pass")
        steps.append("3. No functional changes (only refactoring)")
        steps.append("4. Linting passes with no new warnings")

    elif suggestion_type == "unblock":
        steps.append("1. Blocking issue is resolved or documented")
        steps.append("2. Clear path forward is established")
        steps.append("3. Stakeholders aligned on approach")

    elif suggestion_type == "new":
        steps.append("1. New feature works as specified")
        steps.append("2. Tests cover new functionality")
        steps.append("3. Documentation is updated")
        steps.append("4. No regressions in existing features")

    else:
        steps.append("1. Implementation meets requirements")
        steps.append("2. Tests verify functionality")
        steps.append("3. Code follows project conventions")

    return "\n".join(steps) if steps else "Implementation is complete and tested"


def format_as_scope(suggestion: dict[str, Any]) -> dict[str, Any]:
    """Generate a scope definition from a suggestion."""
    title = suggestion.get("title", "Untitled Task")
    description = suggestion.get("description", "")
    suggested_action = suggestion.get("suggested_action", "")
    related_files = suggestion.get("related_files", [])
    priority = suggestion.get("priority_score", 0.0)

    # Infer effort from priority and action complexity
    effort = infer_effort(suggested_action, priority)

    # Generate goals from action
    goals = extract_goals(suggested_action, description)

    # Generate success criteria
    success_criteria = generate_success_criteria(suggestion.get("type", "new"), related_files)

    return {
        "name": generate_scope_name(title),
        "description": description,
        "goals": goals,
        "success_criteria": success_criteria,
        "estimated_effort": effort,
        "related_areas": related_files[:5] if related_files else []
    }


def infer_effort(action: str, priority: float) -> Literal["small", "medium", "large"]:
    """Infer effort level from action and priority."""
    if not action:
        return "medium"

    action_lower = action.lower()

    # Small effort indicators
    small_indicators = ["fix", "add test", "update", "simple", "minor", "quick"]
    if any(ind in action_lower for ind in small_indicators):
        return "small"

    # Large effort indicators
    large_indicators = ["implement", "refactor", "redesign", "architecture", "complete"]
    if any(ind in action_lower for ind in large_indicators):
        return "large"

    return "medium"


def extract_goals(action: str, description: str) -> list[str]:
    """Extract specific goals from action and description."""
    goals = []

    if action:
        # Split action into sentences/goals
        sentences = [s.strip() for s in action.split(".") if s.strip()]
        goals.extend(sentences[:3])  # Max 3 goals from action

    if description and len(goals) < 3:
        # Add summary as goal if needed
        desc_sentences = [s.strip() for s in description.split(".") if s.strip()]
        if desc_sentences and len(goals) < 3:
            goals.append(desc_sentences[0])

    return goals[:3] if goals else ["Complete the described task"]


def generate_success_criteria(suggestion_type: str, related_files: list[str]) -> list[str]:
    """Generate success criteria based on suggestion type."""
    criteria = []

    if suggestion_type == "continuation":
        criteria = [
            "Feature is fully implemented",
            "All tests pass",
            "Code review approved"
        ]
    elif suggestion_type == "debt":
        criteria = [
            "Technical debt resolved",
            "Test suite passes with no regressions",
            "Code quality improved"
        ]
    elif suggestion_type == "unblock":
        criteria = [
            "Blocking decision made",
            "Implementation path clear",
            "Team aligned on approach"
        ]
    elif suggestion_type == "new":
        criteria = [
            "Feature works as designed",
            "Tests provide good coverage",
            "Documentation updated"
        ]
    else:
        criteria = [
            "Requirements met",
            "Tests pass",
            "Code follows conventions"
        ]

    return criteria


def generate_scope_name(title: str) -> str:
    """Generate a scope-safe name from title."""
    # Lowercase, replace spaces with hyphens, remove special chars
    name = title.lower()
    name = "".join(c if c.isalnum() or c in (" ", "-") else " " for c in name)
    name = name.strip()
    name = "-".join(name.split())[:50]  # Max 50 chars
    return name or "task-scope"


def format_as_cli_command(suggestion: dict[str, Any]) -> str:
    """Generate ready-to-run CLI command from a suggestion."""
    suggestion_type = suggestion.get("type", "new")
    title = suggestion.get("title", "task")
    priority = suggestion.get("priority_score", 0.0)

    # Generate safe slug from title
    slug = generate_scope_name(title)

    # Determine appropriate command
    if suggestion_type == "unblock":
        # Unblock might need discussion first
        return f'/fm:discuss-phase "Unblock {slug} - {title[:60]}"'
    else:
        # Use run-prompt for other types
        worktree_flag = "--worktree" if suggestion_type in ("new", "continuation") else ""
        return f'/fm:run-prompt prompts/generated/{slug}.md {worktree_flag}'.strip()


def get_next_prompt_number(output_dir: Path) -> int:
    """Get the next sequential number for prompt files."""
    if not output_dir.exists():
        return 1

    existing = list(output_dir.glob("*.md"))
    if not existing:
        return 1

    numbers = []
    for f in existing:
        stem = f.stem
        if stem.split("-")[0].isdigit():
            try:
                numbers.append(int(stem.split("-")[0]))
            except (ValueError, IndexError):
                continue

    return max(numbers, default=0) + 1


def generate_prompt_file(
    suggestion: dict[str, Any],
    output_dir: str = "prompts/generated"
) -> str:
    """Write a prompt file and return its path."""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Get next number and generate filename
    next_num = get_next_prompt_number(output_path)
    slug = generate_scope_name(suggestion.get("title", "task"))
    filename = f"{next_num:03d}-{slug}.md"
    file_path = output_path / filename

    # Check if file exists (shouldn't happen with numbering)
    if file_path.exists():
        # Skip to next number
        next_num = get_next_prompt_number(output_path) + 1
        filename = f"{next_num:03d}-{slug}.md"
        file_path = output_path / filename

    # Generate content
    content = format_as_prompt(suggestion)

    # Write file
    file_path.write_text(content)

    return str(file_path)


def generate_workflow(
    suggestions: list[dict[str, Any]],
    output_path: str
) -> str:
    """Generate a workflow.yaml for multiple related suggestions."""
    output = Path(output_path)
    output.parent.mkdir(parents=True, exist_ok=True)

    # Generate prompt file references
    prompt_refs = []
    for i, suggestion in enumerate(suggestions):
        slug = generate_scope_name(suggestion.get("title", f"task-{i}"))
        prompt_refs.append(f"prompts/generated/{slug}.md")

    # Build YAML
    lines = ["workflows:"]
    lines.append("  direction-suggested:")
    lines.append("    base: main")
    lines.append("    branch: direction-suggested-work")
    lines.append("    on_complete:")
    lines.append("      create_pr: true")
    lines.append("")
    lines.append("    prompts:")

    for i, ref in enumerate(prompt_refs):
        task_name = f"task-{i+1}"
        lines.append(f"      {task_name}:")
        lines.append(f"        path: {ref}")
        if i > 0:
            # Add dependency on previous task
            prev_task = f"task-{i}"
            lines.append(f"        after: [{prev_task}]")

    content = "\n".join(lines)

    # Write file
    output.write_text(content)

    return str(output)


def load_suggestions(input_path: str | None) -> list[dict[str, Any]]:
    """Load suggestions from file or run feature_identifier."""
    if input_path:
        # Load from file
        path = Path(input_path)
        if not path.exists():
            print(f"Error: Input file not found: {input_path}", file=sys.stderr)
            sys.exit(1)

        data = json.loads(path.read_text())
        return data if isinstance(data, list) else [data]

    # Run feature_identifier
    print("Running feature_identifier to generate suggestions...")
    try:
        result = subprocess.run(
            ["python", "scripts/feature_identifier.py"],
            capture_output=True,
            text=True,
            check=True
        )
        return json.loads(result.stdout)
    except subprocess.CalledProcessError as e:
        print(f"Error running feature_identifier: {e}", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"Error parsing feature_identifier output: {e}", file=sys.stderr)
        sys.exit(1)


def interactive_mode():
    """Run in interactive mode: show suggestions, prompt for selection."""
    suggestions = load_suggestions(None)

    if not suggestions:
        print("No suggestions found.")
        return

    print("\n=== Feature Suggestions ===\n")
    for i, sugg in enumerate(suggestions, 1):
        print(f"{i}. [{sugg.get('type', 'new').upper()}] {sugg.get('title', 'Untitled')}")
        print(f"   Priority: {sugg.get('priority_score', 0):.2f}")
        print(f"   {sugg.get('description', '')[:80]}...")
        print()

    # Prompt for selection
    try:
        selection = input("Enter suggestion numbers to format (comma-separated, or 'all'): ").strip()
    except (EOFError, KeyboardInterrupt):
        print("\nCancelled.")
        return

    if selection.lower() == "all":
        selected = suggestions
    else:
        indices = [int(x.strip()) - 1 for x in selection.split(",")]
        selected = [suggestions[i] for i in indices if 0 <= i < len(suggestions)]

    # Prompt for format
    print("\nAvailable formats:")
    print("  1. prompt    - Generate prompt files")
    print("  2. scope     - Generate scope definitions")
    print("  3. cli       - Generate CLI commands")
    print("  4. workflow  - Generate workflow.yaml")
    print("  5. all       - Generate all formats")

    try:
        format_choice = input("Select format (1-5): ").strip()
    except (EOFError, KeyboardInterrupt):
        print("\nCancelled.")
        return

    format_map = {
        "1": "prompt",
        "2": "scope",
        "3": "cli",
        "4": "workflow",
        "5": "all"
    }
    selected_format = format_map.get(format_choice, "all")

    # Generate outputs
    generate_outputs(selected, selected_format)


def generate_outputs(
    suggestions: list[dict[str, Any]],
    format_type: str,
    output_dir: str = "prompts/generated"
):
    """Generate outputs based on format type."""
    if not suggestions:
        print("No suggestions to format.")
        return

    if format_type in ("prompt", "all"):
        print("\nGenerating prompt files...")
        for sugg in suggestions:
            path = generate_prompt_file(sugg, output_dir)
            print(f"  Created: {path}")

    if format_type in ("scope", "all"):
        print("\nGenerating scope definitions...")
        scopes_dir = Path(output_dir) / "scopes"
        scopes_dir.mkdir(parents=True, exist_ok=True)

        for sugg in suggestions:
            scope = format_as_scope(sugg)
            name = scope["name"]
            path = scopes_dir / f"{name}.json"
            path.write_text(json.dumps(scope, indent=2))
            print(f"  Created: {path}")

    if format_type in ("cli", "all"):
        print("\nGenerating CLI commands...")
        commands_path = Path(output_dir) / "commands.txt"
        commands = []
        for sugg in suggestions:
            cmd = format_as_cli_command(sugg)
            commands.append(cmd)
        commands_path.write_text("\n".join(commands))
        print(f"  Created: {commands_path}")
        print("\nCommands:")
        for cmd in commands:
            print(f"  {cmd}")

    if format_type in ("workflow", "all") and len(suggestions) > 1:
        print("\nGenerating workflow.yaml...")
        workflow_path = Path(output_dir) / "workflow.yaml"
        path = generate_workflow(suggestions, str(workflow_path))
        print(f"  Created: {path}")


def main():
    parser = argparse.ArgumentParser(
        description="Format feature suggestions for founder-mode",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/pm_formatter.py --interactive
  python scripts/pm_formatter.py --input suggestions.json --format all
  python scripts/pm_formatter.py --input suggestions.json --format prompt
  python scripts/pm_formatter.py --input suggestions.json --format cli
        """
    )

    parser.add_argument(
        "--input",
        help="Path to suggestions JSON file (default: run feature_identifier)"
    )
    parser.add_argument(
        "--format",
        choices=["prompt", "scope", "cli", "workflow", "all"],
        default="all",
        help="Output format (default: all)"
    )
    parser.add_argument(
        "--output-dir",
        default="prompts/generated",
        help="Where to write generated files (default: prompts/generated)"
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="Interactive mode: select suggestions and format"
    )

    args = parser.parse_args()

    if args.interactive:
        interactive_mode()
    else:
        suggestions = load_suggestions(args.input)
        generate_outputs(suggestions, args.format, args.output_dir)


if __name__ == "__main__":
    main()
